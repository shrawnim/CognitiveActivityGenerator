{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall torch torchvision torchaudio transformers numpy pandas torch datasets openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-JA67BYa1yz",
        "outputId": "26c9ead8-9021-4d48-fd33-fbbc1b204c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting datasets\n",
            "  Using cached datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.63.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
            "  Using cached pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
            "  Using cached huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting packaging>=20.0 (from transformers)\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers)\n",
            "  Using cached safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Using cached pyarrow-19.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Using cached aiohttp-3.11.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai)\n",
            "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "rCnbKRhTZWyr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "md2HFM2Ijv-B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/cleaned_cognitive_activities.csv\")\n",
        "\n",
        "# Identify category columns (Ensure unique column names)\n",
        "category_columns = [\"Memory\", \"Reasoning\", \"Association\",\"Reasoning\"]\n",
        "\n",
        "# Extract cognitive skills\n",
        "def get_skills(row):\n",
        "    return \", \".join([col for col in category_columns if row[col] == \"Yes\"]) or \"General Cognitive Skills\"\n",
        "\n",
        "df['skills_text'] = df.apply(get_skills, axis=1)\n",
        "\n",
        "# Create structured input for fine-tuning\n",
        "df['input_text'] = (\"Generate an activity for Zone: \" + df['Zone'] +\n",
        "                    \", Age Range: \" + df['Age'] +\n",
        "                    \", Cognitive Skills: \" + df['skills_text'])\n",
        "\n",
        "# Generate default fields\n",
        "df['Activity Name'] = df['skills_text'] + \" Challenge\"\n",
        "df['description'] = \"An engaging cognitive activity focused on \" + df['skills_text'].fillna(\"multiple skills\") + \".\"\n",
        "df['Instructions'] = \"Step 1: Read the instructions carefully.\\nStep 2: Follow the steps based on \" + df['skills_text'] + \".\\nStep 3: Complete the task and analyze your performance.\"\n",
        "df['time_required'] = df.apply(lambda x: str(np.random.randint(12, 20)) + \" minutes\", axis=1)\n",
        "\n",
        "# Assign Zone & Objective\n",
        "df['zone'] = df['Zone'].fillna(\"yellow\")\n",
        "df['objective'] = df['skills_text'].apply(lambda skills: \"Improve \" + skills.lower() + \" skills.\")\n",
        "\n",
        "# Format dataset for training\n",
        "df['output_text'] = (\n",
        "    \"Activity: \" + df['Activity Name'] +\n",
        "    \"\\nDescription: \" + df['description'] +\n",
        "    \"\\nInstructions: \" + df['Instructions'] +\n",
        "    \"\\nMaterials Required: None\" +\n",
        "    \"\\nTime Required: \" + df['time_required'] +\n",
        "    \"\\nZone: \" + df['zone'] +\n",
        "    \"\\nObjective: \" + df['objective']\n",
        ")\n",
        "\n",
        "# Save dataset\n",
        "df[['input_text', 'output_text']].to_json(\"/content/activity_dataset.json\", orient=\"records\", lines=True)\n",
        "\n",
        "print(\"‚úÖ Dataset prepared successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuDM8522Bpz8",
        "outputId": "1a4c83f0-ea4b-43a0-e3b9-e56de65e4dba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset prepared successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/activity_dataset.json\", \"r\") as f:\n",
        "    for i in range(3):  # Print the first 3 examples\n",
        "        print(json.loads(f.readline()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfL7SCfdLn_J",
        "outputId": "36549cc2-ed27-4677-ea1a-64a2bc32849f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_text': 'Generate an activity for Zone: green, Age Range: 5-13, Cognitive Skills: Reasoning, Reasoning', 'output_text': 'Activity: Reasoning, Reasoning Challenge\\nDescription: An engaging cognitive activity focused on Reasoning, Reasoning.\\nInstructions: Step 1: Read the instructions carefully.\\nStep 2: Follow the steps based on Reasoning, Reasoning.\\nStep 3: Complete the task and analyze your performance.\\nMaterials Required: None\\nTime Required: 15 minutes\\nZone: green\\nObjective: Improve reasoning, reasoning skills.'}\n",
            "{'input_text': 'Generate an activity for Zone: yellow, Age Range: 5-13, Cognitive Skills: Memory', 'output_text': 'Activity: Memory Challenge\\nDescription: An engaging cognitive activity focused on Memory.\\nInstructions: Step 1: Read the instructions carefully.\\nStep 2: Follow the steps based on Memory.\\nStep 3: Complete the task and analyze your performance.\\nMaterials Required: None\\nTime Required: 17 minutes\\nZone: yellow\\nObjective: Improve memory skills.'}\n",
            "{'input_text': 'Generate an activity for Zone: green, Age Range: 5-13, Cognitive Skills: Reasoning, Reasoning', 'output_text': 'Activity: Reasoning, Reasoning Challenge\\nDescription: An engaging cognitive activity focused on Reasoning, Reasoning.\\nInstructions: Step 1: Read the instructions carefully.\\nStep 2: Follow the steps based on Reasoning, Reasoning.\\nStep 3: Complete the task and analyze your performance.\\nMaterials Required: None\\nTime Required: 18 minutes\\nZone: green\\nObjective: Improve reasoning, reasoning skills.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/activity_dataset.json\", split=\"train\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-small\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_data(batch):\n",
        "     if \"input_text\" not in batch or \"output_text\" not in batch:\n",
        "        print(\"‚ùå Missing 'input_text' or 'output_text':\", batch)  # Debugging\n",
        "        return batch  # Skip invalid data\n",
        "\n",
        "     inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "     outputs = tokenizer(batch[\"output_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "     batch[\"input_ids\"] = inputs[\"input_ids\"]\n",
        "     batch[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
        "     batch[\"labels\"] = outputs[\"input_ids\"]\n",
        "\n",
        "     return batch\n",
        "\n",
        "dataset = dataset.map(tokenize_data)\n",
        "\n",
        "# Load model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-small\")\n",
        "\n",
        "# Training settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5_activity_generator\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"no\",\n",
        "    fp16=True,  # Use GPU if available\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=dataset)\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"./activity_generator_model\")\n",
        "tokenizer.save_pretrained(\"./activity_generator_model\")\n",
        "\n",
        "print(\"‚úÖ Model training complete! Saved as 'activity_generator_model'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "2338bcbe519e4d1f930f6106c86efc75",
            "ebf3865699034e4f9fe7befc8a9a0d1b",
            "76113b066dc84a3eab9119f7aa86f556",
            "8aa863c8fb964276aaa55475562fa8b9",
            "4009e097309a428f8f6f6c5a1a351fc4",
            "ed0acb4e0f1d4a84a3015da4b39bafdf",
            "590fbaf1461f48daa786c936d90edd81",
            "e1dd6bd7f63d44d798e0648512b84d2a",
            "7574b99352ef4c0daa44283ce95c69a5",
            "632e258917fa4c9490e219859d71e83d",
            "1b57380dd818443b96a62646b84d5dc3"
          ]
        },
        "id": "9g-iJRd1B8lL",
        "outputId": "7a17272b-53c5-4c83-acec-f1a5318b0697"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2338bcbe519e4d1f930f6106c86efc75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [65/65 37:55, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model training complete! Saved as 'activity_generator_model'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine-tuned model\n",
        "model_path = \"./activity_generator_model\"\n",
        "try:\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error loading model:\", e)\n",
        "\n",
        "def generate_activity(cognitive_skills, zone):\n",
        "    \"\"\"Generate a cognitive activity based on skills & zone.\"\"\"\n",
        "    prompt = (f\"in English,generate an activity for Cognitive Skills: {cognitive_skills}, The response should include: Activity name, Description, \"\n",
        "              f\"Instructions, Materials Required, Time Required, Zone, and Objective.\")\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    print(inputs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**inputs, max_length=512, num_return_sequences=1,temperature=0.7, top_p=0.9,\n",
        "            do_sample=True)\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    if not generated_text.strip():\n",
        "          generated_text = (\n",
        "                \"Activity: Memory Puzzle\\n\"\n",
        "                \"Description: A fun game to improve memory skills.\\n\"\n",
        "                \"Instructions: Match the cards to test memory.\\n\"\n",
        "                \"Materials Required: Cards\\n\"\n",
        "                \"Time Required: 15 minutes\\n\"\n",
        "                \"Objective: Enhance memory retention.\"\n",
        "            )\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage:\n",
        "print(generate_activity(\"Memory, Reasoning\", \"Green\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jR2On6fCczE",
        "outputId": "a1c32c26-eaa7-4fd5-eb3f-9089741e2136"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully!\n",
            "{'input_ids': tensor([[   16,  1566,     6,   729,    49,   342,    46,  1756,    21, 31109,\n",
            "         19559,    10, 19159,     6, 21272,    53,     6,    37,  1773,   225,\n",
            "           560,    10, 22536,   564,     6,  7726,     6, 21035,     7,     6,\n",
            "         16158, 31377,     6,  2900, 31377,     6, 11628,     6,    11, 27919,\n",
            "             5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            ", the website. this website.in English,return. this article.in English,in English,reflect. this page. this page. | Michael.? ??????????????????????????\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (f\"Generate an activity in English for Cognitive Skills: memory Zone: green \"\n",
        "              f\"The response must be in English and 7 lines must have one line for each: Activity name, Description, Instructions, \"\n",
        "              f\"Materials Required, Time Required, Zone, and Objective.\")\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "\n",
        "with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,  # Encourage creativity\n",
        "            top_p=0.9,  # Use nucleus sampling\n",
        "            do_sample=True  # Enable sampling\n",
        "        )\n",
        "\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"üîπ Debug: Raw Model Output ->\", repr(generated_text))  # Debugging step\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIaPOGDacLAT",
        "outputId": "f8656bd8-f9b4-495c-a157-53938d8e1429"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Debug: Raw Model Output -> \"thist 2016,ulphLL: TI, Pres' goall  canyon'FT-y\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r activity_generator_model.zip /content/activity_generator_model\n",
        "from google.colab import files\n",
        "files.download(\"activity_generator_model.zip\")"
      ],
      "metadata": {
        "id": "k8B_nUrIs8gX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aadcc823-2e97-454d-bea0-2dd1a558e250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['README.md', 'anscombe.json', 'mnist_test.csv', 'mnist_train_small.csv', 'california_housing_test.csv', 'california_housing_train.csv']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2338bcbe519e4d1f930f6106c86efc75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebf3865699034e4f9fe7befc8a9a0d1b",
              "IPY_MODEL_76113b066dc84a3eab9119f7aa86f556",
              "IPY_MODEL_8aa863c8fb964276aaa55475562fa8b9"
            ],
            "layout": "IPY_MODEL_4009e097309a428f8f6f6c5a1a351fc4"
          }
        },
        "ebf3865699034e4f9fe7befc8a9a0d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0acb4e0f1d4a84a3015da4b39bafdf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_590fbaf1461f48daa786c936d90edd81",
            "value": "Map:‚Äá100%"
          }
        },
        "76113b066dc84a3eab9119f7aa86f556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1dd6bd7f63d44d798e0648512b84d2a",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7574b99352ef4c0daa44283ce95c69a5",
            "value": 99
          }
        },
        "8aa863c8fb964276aaa55475562fa8b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_632e258917fa4c9490e219859d71e83d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1b57380dd818443b96a62646b84d5dc3",
            "value": "‚Äá99/99‚Äá[00:00&lt;00:00,‚Äá276.23‚Äáexamples/s]"
          }
        },
        "4009e097309a428f8f6f6c5a1a351fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed0acb4e0f1d4a84a3015da4b39bafdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "590fbaf1461f48daa786c936d90edd81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1dd6bd7f63d44d798e0648512b84d2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7574b99352ef4c0daa44283ce95c69a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "632e258917fa4c9490e219859d71e83d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b57380dd818443b96a62646b84d5dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}